{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-19T01:22:09.077847Z",
     "start_time": "2026-02-19T01:22:09.074696Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"sklearn\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T01:22:09.105204Z",
     "start_time": "2026-02-19T01:22:09.095329Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(r\"/Users/loganheydt/Documents/GitHub/Kaggle_competitions/Wine_Quality/data/train.csv\")",
   "id": "29914df7631d5e67",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T01:22:09.125017Z",
     "start_time": "2026-02-19T01:22:09.118771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"quality\"]\n",
    ")"
   ],
   "id": "1a99a663cc6237a9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T01:22:09.143946Z",
     "start_time": "2026-02-19T01:22:09.139632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_thresholds(scores, thresholds, min_class):\n",
    "    return (np.digitize(scores, thresholds) + min_class).astype(int)\n",
    "\n",
    "def optimize_thresholds(scores, y_true, classes, n_iter=30):\n",
    "    \"\"\"\n",
    "    Coordinate-descent threshold optimizer to maximize QWK.\n",
    "    thresholds length = len(classes)-1\n",
    "    \"\"\"\n",
    "    classes = list(classes)\n",
    "    min_c, max_c = classes[0], classes[-1]\n",
    "    k = len(classes)\n",
    "\n",
    "    qs = np.linspace(0, 1, k+1)[1:-1]\n",
    "    thr = np.quantile(scores, qs)\n",
    "\n",
    "    def qwk_for(thr_):\n",
    "        pred = apply_thresholds(scores, thr_, min_c)\n",
    "        pred = np.clip(pred, min_c, max_c)\n",
    "        return cohen_kappa_score(y_true, pred, weights=\"quadratic\")\n",
    "\n",
    "    best = qwk_for(thr)\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        improved = False\n",
    "        for i in range(len(thr)):\n",
    "            cand = np.quantile(scores, np.linspace(0.05, 0.95, 41))\n",
    "            lo = thr[i-1] if i > 0 else -np.inf\n",
    "            hi = thr[i+1] if i < len(thr)-1 else np.inf\n",
    "            cand = cand[(cand > lo) & (cand < hi)]\n",
    "            if len(cand) == 0:\n",
    "                continue\n",
    "\n",
    "            local_best_thr = thr[i]\n",
    "            local_best = best\n",
    "\n",
    "            for v in cand:\n",
    "                thr_try = thr.copy()\n",
    "                thr_try[i] = v\n",
    "                val = qwk_for(thr_try)\n",
    "                if val > local_best:\n",
    "                    local_best = val\n",
    "                    local_best_thr = v\n",
    "\n",
    "            if local_best > best:\n",
    "                thr[i] = local_best_thr\n",
    "                best = local_best\n",
    "                improved = True\n",
    "\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "    return thr, best\n",
    "\n",
    "def make_monotone_constraints(X, monotone_dict):\n",
    "    \"\"\"\n",
    "    X: feature DF\n",
    "    monotone_dict: e.g. {\"alcohol\": 1} (1 = increasing, -1 = decreasing, 0 = none)\n",
    "    Returns constraints string for xgboost: \"(0,1,0,...)\"\n",
    "    \"\"\"\n",
    "    cons = []\n",
    "    for c in X.columns:\n",
    "        cons.append(int(monotone_dict.get(c, 0)))\n",
    "    return \"(\" + \",\".join(map(str, cons)) + \")\""
   ],
   "id": "21b035c7a17e4e65",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T01:22:09.162673Z",
     "start_time": "2026-02-19T01:22:09.156352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stacked_oof_threshold_cv(\n",
    "    df,\n",
    "    target_col=\"quality\",\n",
    "    drop_cols=None,\n",
    "    n_splits=5,\n",
    "    seed=42,\n",
    "    monotone_feature=\"alcohol\"\n",
    "):\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    if \"id\" in df.columns and \"id\" not in drop_cols:\n",
    "        drop_cols = drop_cols + [\"id\"]\n",
    "\n",
    "    y = df[target_col].astype(int).values\n",
    "    X = df.drop(columns=[target_col] + drop_cols, errors=\"ignore\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    classes = sorted(np.unique(y).tolist())\n",
    "    min_c, max_c = classes[0], classes[-1]\n",
    "\n",
    "    # --- monotone constraints (only alcohol increasing by default) ---\n",
    "    mono = {}\n",
    "    if monotone_feature in X.columns:\n",
    "        mono[monotone_feature] = 1\n",
    "    monotone_constraints = make_monotone_constraints(X, mono)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Store OOF predictions for each base model\n",
    "    oof_xgb_mono = np.zeros(len(X), dtype=float)\n",
    "    oof_rf = np.zeros(len(X), dtype=float)\n",
    "    oof_ridge = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (tr, va) in enumerate(kf.split(X), start=1):\n",
    "        X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "        y_tr, y_va = y[tr], y[va]\n",
    "\n",
    "        # --- Base 1: XGBRegressor with monotone alcohol ---\n",
    "        xgb_mono = XGBRegressor(\n",
    "            n_estimators=900,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            monotone_constraints=monotone_constraints,\n",
    "        )\n",
    "        xgb_mono.fit(X_tr, y_tr)\n",
    "        oof_xgb_mono[va] = xgb_mono.predict(X_va)\n",
    "\n",
    "        # --- Base 2: RandomForest ---\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=600,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            min_samples_leaf=2\n",
    "        )\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        oof_rf[va] = rf.predict(X_va)\n",
    "\n",
    "        # --- Base 3: Ridge on standardized-ish inputs (Ridge can help as a “linear anchor”) ---\n",
    "        # Ridge needs numeric matrix; NaNs -> fill (simple median impute)\n",
    "        X_tr_r = X_tr.copy()\n",
    "        X_va_r = X_va.copy()\n",
    "        med = X_tr_r.median(numeric_only=True)\n",
    "        X_tr_r = X_tr_r.fillna(med)\n",
    "        X_va_r = X_va_r.fillna(med)\n",
    "\n",
    "        ridge = Ridge(alpha=1.0, random_state=seed)\n",
    "        ridge.fit(X_tr_r, y_tr)\n",
    "        oof_ridge[va] = ridge.predict(X_va_r)\n",
    "\n",
    "        # quick fold check (optional)\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_va, oof_xgb_mono[va]))\n",
    "        fold_scores.append({\"fold\": fold, \"xgb_mono_rmse\": fold_rmse, \"n_val\": len(va)})\n",
    "\n",
    "    fold_df = pd.DataFrame(fold_scores)\n",
    "\n",
    "    # ---- STACKING: meta-model trained on OOF predictions ----\n",
    "    oof_stack_X = np.vstack([oof_xgb_mono, oof_rf, oof_ridge]).T\n",
    "\n",
    "    meta = Ridge(alpha=1.0, random_state=seed)\n",
    "    meta.fit(oof_stack_X, y)\n",
    "    oof_meta_scores = meta.predict(oof_stack_X)  # latent continuous score\n",
    "\n",
    "    # ---- 1️⃣ Joint thresholds across folds: optimize on OOF predictions ----\n",
    "    thr, best_oof_qwk = optimize_thresholds(oof_meta_scores, y, classes=classes, n_iter=40)\n",
    "\n",
    "    # Evaluate with those single global thresholds\n",
    "    preds = apply_thresholds(oof_meta_scores, thr, min_c)\n",
    "    preds = np.clip(preds, min_c, max_c)\n",
    "    qwk = cohen_kappa_score(y, preds, weights=\"quadratic\")\n",
    "\n",
    "    summary = {\n",
    "        \"n_splits\": n_splits,\n",
    "        \"classes\": classes,\n",
    "        \"monotone_constraints_used\": monotone_constraints,\n",
    "        \"OOF_QWK_global_thresholds\": float(qwk),\n",
    "        \"OOF_QWK_during_optimization\": float(best_oof_qwk),\n",
    "        \"thresholds\": thr.tolist(),\n",
    "    }\n",
    "\n",
    "    return summary, fold_df, pd.DataFrame({\n",
    "        \"y_true\": y,\n",
    "        \"oof_xgb_mono\": oof_xgb_mono,\n",
    "        \"oof_rf\": oof_rf,\n",
    "        \"oof_ridge\": oof_ridge,\n",
    "        \"oof_meta_score\": oof_meta_scores,\n",
    "        \"oof_pred_class\": preds\n",
    "    })"
   ],
   "id": "eeed213e6604a5c3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T01:22:24.651042Z",
     "start_time": "2026-02-19T01:22:09.174589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary, fold_df, oof_df = stacked_oof_threshold_cv(\n",
    "    df_train,\n",
    "    target_col=\"quality\",\n",
    "    drop_cols=[\"id\"],\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "thresholds = summary[\"thresholds\"]"
   ],
   "id": "752ba9266cc12a71",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T01:22:24.683223Z",
     "start_time": "2026-02-19T01:22:24.675813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def make_monotone_constraints(X: pd.DataFrame, monotone_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    X: feature DF (column order matters)\n",
    "    monotone_dict: e.g. {\"alcohol\": 1} (1=increasing, -1=decreasing, 0=none)\n",
    "    Returns an xgboost constraint string like \"(0,1,0,...)\"\n",
    "    \"\"\"\n",
    "    cons = [int(monotone_dict.get(c, 0)) for c in X.columns]\n",
    "    return \"(\" + \",\".join(map(str, cons)) + \")\"\n",
    "\n",
    "\n",
    "def apply_thresholds(scores: np.ndarray, thresholds, min_class: int) -> np.ndarray:\n",
    "    thresholds = np.asarray(thresholds, dtype=float)\n",
    "    return (np.digitize(scores, thresholds) + min_class).astype(int)\n",
    "\n",
    "\n",
    "def fit_final_stacked_model(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"quality\",\n",
    "    drop_cols=None,\n",
    "    seed: int = 42,\n",
    "    monotone_feature: str = \"alcohol\",\n",
    "    thresholds=None,\n",
    "    xgb_params: dict = None,\n",
    "    rf_params: dict = None,\n",
    "    meta_params: dict = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits base models + meta model on ALL provided data (typically df_train).\n",
    "    Uses provided thresholds (recommended: thresholds learned from OOF CV on df_train).\n",
    "\n",
    "    Returns a model_bundle dict to be used with predict_final().\n",
    "    \"\"\"\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    if \"id\" in df.columns and \"id\" not in drop_cols:\n",
    "        drop_cols = drop_cols + [\"id\"]\n",
    "\n",
    "    y = df[target_col].astype(int).values\n",
    "    X = df.drop(columns=[target_col] + drop_cols, errors=\"ignore\").copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    classes = sorted(np.unique(y).tolist())\n",
    "    min_c, max_c = classes[0], classes[-1]\n",
    "\n",
    "    # --- monotone constraints (alcohol increasing by default) ---\n",
    "    mono = {}\n",
    "    if monotone_feature in X.columns:\n",
    "        mono[monotone_feature] = 1\n",
    "    monotone_constraints = make_monotone_constraints(X, mono)\n",
    "\n",
    "    # --- default params (you can override) ---\n",
    "    if xgb_params is None:\n",
    "        xgb_params = dict(\n",
    "            n_estimators=900,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    if rf_params is None:\n",
    "        rf_params = dict(\n",
    "            n_estimators=600,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            min_samples_leaf=2\n",
    "        )\n",
    "    if meta_params is None:\n",
    "        meta_params = dict(alpha=1.0, random_state=seed)\n",
    "\n",
    "    # --- Base 1: XGBRegressor with monotone constraints ---\n",
    "    xgb_mono = XGBRegressor(\n",
    "        **xgb_params,\n",
    "        monotone_constraints=monotone_constraints,\n",
    "    )\n",
    "    xgb_mono.fit(X, y)\n",
    "    pred_xgb = xgb_mono.predict(X)\n",
    "\n",
    "    # --- Base 2: RandomForest ---\n",
    "    rf = RandomForestRegressor(**rf_params)\n",
    "    rf.fit(X, y)\n",
    "    pred_rf = rf.predict(X)\n",
    "\n",
    "    # --- Base 3: Ridge (needs imputation for NaNs) ---\n",
    "    medians = X.median(numeric_only=True)\n",
    "    X_r = X.fillna(medians)\n",
    "    ridge = Ridge(alpha=1.0, random_state=seed)\n",
    "    ridge.fit(X_r, y)\n",
    "    pred_ridge = ridge.predict(X_r)\n",
    "\n",
    "    # --- Meta model trained on base predictions ---\n",
    "    stack_train = np.vstack([pred_xgb, pred_rf, pred_ridge]).T\n",
    "    meta = Ridge(**meta_params)\n",
    "    meta.fit(stack_train, y)\n",
    "\n",
    "    # If thresholds not provided, we default to quantiles (OK for quick use, but prefer OOF-learned thresholds)\n",
    "    if thresholds is None:\n",
    "        qs = np.linspace(0, 1, len(classes)+1)[1:-1]\n",
    "        thresholds = np.quantile(meta.predict(stack_train), qs)\n",
    "\n",
    "    model_bundle = {\n",
    "        \"classes\": classes,\n",
    "        \"min_c\": min_c,\n",
    "        \"max_c\": max_c,\n",
    "        \"thresholds\": np.asarray(thresholds, dtype=float),\n",
    "\n",
    "        \"feature_cols\": X.columns.tolist(),\n",
    "        \"medians_for_ridge\": medians,\n",
    "\n",
    "        \"monotone_constraints_used\": monotone_constraints,\n",
    "\n",
    "        \"xgb_mono\": xgb_mono,\n",
    "        \"rf\": rf,\n",
    "        \"ridge\": ridge,\n",
    "        \"meta\": meta,\n",
    "    }\n",
    "    return model_bundle\n",
    "\n",
    "\n",
    "def predict_final(model_bundle: dict, df_new: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Predicts:\n",
    "      - pred_class: integer labels in [min_c, max_c]\n",
    "      - scores: continuous latent score from meta model\n",
    "    \"\"\"\n",
    "    X_new = df_new[model_bundle[\"feature_cols\"]].copy()\n",
    "    X_new = X_new.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    p1 = model_bundle[\"xgb_mono\"].predict(X_new)\n",
    "    p2 = model_bundle[\"rf\"].predict(X_new)\n",
    "\n",
    "    Xr = X_new.fillna(model_bundle[\"medians_for_ridge\"])\n",
    "    p3 = model_bundle[\"ridge\"].predict(Xr)\n",
    "\n",
    "    stack = np.vstack([p1, p2, p3]).T\n",
    "    scores = model_bundle[\"meta\"].predict(stack)\n",
    "\n",
    "    pred_class = apply_thresholds(scores, model_bundle[\"thresholds\"], model_bundle[\"min_c\"])\n",
    "    pred_class = np.clip(pred_class, model_bundle[\"min_c\"], model_bundle[\"max_c\"]).astype(int)\n",
    "\n",
    "    return pred_class, scores"
   ],
   "id": "110f149bb7fede74",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T01:22:43.458605Z",
     "start_time": "2026-02-19T01:22:24.685764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"quality\"]\n",
    ")\n",
    "\n",
    "# 1) Run your OOF stacking + threshold optimization on df_train to get thresholds\n",
    "summary, fold_df, oof_df = stacked_oof_threshold_cv(df_train, target_col=\"quality\", drop_cols=[\"id\"])\n",
    "thr = summary[\"thresholds\"]\n",
    "\n",
    "# 2) Fit final model on df_train using those thresholds\n",
    "final_model = fit_final_stacked_model(df_train, target_col=\"quality\", drop_cols=[\"id\"], thresholds=thr)\n",
    "\n",
    "# 3) Predict on df_test\n",
    "pred_test, score_test = predict_final(final_model, df_test)\n",
    "\n",
    "qwk_test = cohen_kappa_score(df_test[\"quality\"], pred_test, weights=\"quadratic\")\n",
    "print(\"Test QWK:\", qwk_test)"
   ],
   "id": "8f5086b7cbe38850",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test QWK: 0.2602667625160814\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
